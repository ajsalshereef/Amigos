{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CNN_palpx.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajsalshereef/Amigos/blob/master/CNN_palpx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqbk_2hiks4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report \n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbU90LuRkzzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0beca977-c121-465c-a724-0ea51490d6a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap2RfPJBlCDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/Cat vs dog/train.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olg_eSKxks4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the dog dataset.Only taking 100 datapoints from dog class\n",
        "folder = 'train/'\n",
        "photos, labels = list(), list()\n",
        "for i in range(100):\n",
        "    # define filename\n",
        "    filename = folder + 'dog.' + str(i) + '.jpg'\n",
        "    # load the image \n",
        "    image = plt.imread(filename)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, dsize=(200, 200), interpolation=cv2.INTER_CUBIC)\n",
        "    image = image.flatten()\n",
        "    photos.append(image)\n",
        "    labels.append(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpCveMecks4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the cat dataset. Only taking 100 datapoints from cat class\n",
        "folder = 'train/'\n",
        "for i in range(100):\n",
        "    # define filename\n",
        "    filename = folder + 'cat.' + str(i) + '.jpg'\n",
        "    # load the image \n",
        "    image = plt.imread(filename)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    image = cv2.resize(image, dsize=(200, 200), interpolation=cv2.INTER_CUBIC)\n",
        "    image = image.flatten()\n",
        "    photos.append(image)\n",
        "    labels.append(1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOLWJkGTks4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "photos = np.array(photos).reshape(200,200*200) #Reshaping the photos"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMVIW9Yyks4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.array(labels).reshape(200,1) #Reshaping the labels\n",
        "data = np.hstack((photos,labels)) # Stacking the photos and labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly1SamVMks4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(data) # Shuffling the data for creating the train and test data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df1pF0JWks41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data  = data[0:160]\n",
        "test_data  = data[160:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oamwdIzgks46",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb2d4586-c8d5-45b9-a980-35de25d2c65b"
      },
      "source": [
        "np.unique(train_data[:,-1],return_counts =True) # Confirming the shuffling"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([74, 86]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhLnLwXks5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will initialise the filter of given size\n",
        "def initializeFilter(size, scale = 1.0):\n",
        "    stddev = scale/np.sqrt(np.prod(size))\n",
        "    return np.random.normal(loc = 0, scale = stddev, size = size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pclYk54bks5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will initialise the weights in neural net\n",
        "def initializeWeight(size):\n",
        "    return np.random.standard_normal(size=size) * 0.01"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwaWQDBZks5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This function will assign the class depending on the output from softmax\n",
        "def nanargmax(arr):\n",
        "    idx = np.nanargmax(arr)\n",
        "    idxs = np.unravel_index(idx, arr.shape)\n",
        "    return idxs    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzpRVmFhks5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will convolve the filter over the image with given stride\n",
        "def convolution(image, filt, bias, s=1):\n",
        "    (n_f, n_c_f, f, _) = filt.shape # filter dimensions\n",
        "    n_c, in_dim, _ = image.shape # image dimensions\n",
        "    \n",
        "    out_dim = int((in_dim - f)/s)+1 # calculate output dimensions\n",
        "    \n",
        "    assert n_c == n_c_f, \"Dimensions of filter must match dimensions of input image\"\n",
        "    \n",
        "    out = np.zeros((n_f,out_dim,out_dim))\n",
        "    \n",
        "    # convolve the filter over every part of the image, adding the bias at each step. \n",
        "    for curr_f in range(n_f):\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= in_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= in_dim:\n",
        "                out[curr_f, out_y, out_x] = np.sum(filt[curr_f] * image[:,curr_y:curr_y+f, curr_x:curr_x+f]) + bias[curr_f]\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "        \n",
        "    return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByFS8dSZks5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the maximum value of image pixle part with given filter size and stride\n",
        "def maxpool(image, f=2, s=2):\n",
        "    n_c, h_prev, w_prev = image.shape\n",
        "    \n",
        "    h = int((h_prev - f)/s)+1 # calculate output dimensions\n",
        "    w = int((w_prev - f)/s)+1\n",
        "    \n",
        "    downsampled = np.zeros((n_c, h, w))\n",
        "    for i in range(n_c):\n",
        "        # slide maxpool window over each part of the image and assign the max value at each step to the output\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= h_prev:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= w_prev:\n",
        "                downsampled[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "    return downsampled"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEFgeegPks5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    out = np.exp(X)\n",
        "    return out/np.sum(out)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4LC4694ks5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categoricalCrossEntropy(probs, label):\n",
        "    return -np.sum(label * np.log(probs))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efEdMRY1ks5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backpropagation through a convolutional layer.\n",
        "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
        "    (n_f, n_c, f, _) = filt.shape\n",
        "    (_, orig_dim, _) = conv_in.shape\n",
        "    ## initialize derivatives\n",
        "    dout = np.zeros(conv_in.shape) \n",
        "    dfilt = np.zeros(filt.shape)\n",
        "    dbias = np.zeros((n_f,1))\n",
        "    for curr_f in range(n_f):\n",
        "        # loop through all filters\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "                #loss gradient of filter (used to update the filter)\n",
        "                dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
        "                # loss gradient of the input to the convolution operation (conv1 in the case of this network)\n",
        "                dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f] \n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1\n",
        "        # loss gradient of the bias\n",
        "        dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
        "    return dout, dfilt, dbias"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjCXs8paks54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Backpropagation through maxpool layer\n",
        "def maxpoolBackward(dpool, orig, f, s):\n",
        "    (n_c, orig_dim, _) = orig.shape\n",
        "    dout = np.zeros(orig.shape)\n",
        "    for curr_c in range(n_c):\n",
        "        curr_y = out_y = 0\n",
        "        while curr_y + f <= orig_dim:\n",
        "            curr_x = out_x = 0\n",
        "            while curr_x + f <= orig_dim:\n",
        "                # obtain index of largest value in input for current window\n",
        "                (a, b) = nanargmax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "                dout[curr_c, curr_y+a, curr_x+b] = dpool[curr_c, out_y, out_x]\n",
        "                \n",
        "                curr_x += s\n",
        "                out_x += 1\n",
        "            curr_y += s\n",
        "            out_y += 1  \n",
        "    return dout"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-GJIm6vks5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv(image, label, params, conv_s, pool_f, pool_s):\n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params \n",
        "    #Forward pass\n",
        "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
        "    conv1[conv1<=0] = 0 # pass through ReLU non-linearity\n",
        "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
        "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
        "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
        "    (nf2, dim2, _) = pooled.shape\n",
        "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
        "    z = w3.dot(fc) + b3 # first dense layer\n",
        "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
        "    out = w4.dot(z) + b4 # second dense layer\n",
        "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
        "    #calculating the Loss \n",
        "    loss = categoricalCrossEntropy(probs, label) # categorical cross-entropy loss\n",
        "    #Backward pass\n",
        "    dout = probs - label # derivative of loss w.r.t. final dense layer output\n",
        "    dw4 = dout.dot(z.T) # loss gradient of final dense layer weights\n",
        "    db4 = np.sum(dout, axis = 1).reshape(b4.shape) # loss gradient of final dense layer biases\n",
        "    dz = w4.T.dot(dout) # loss gradient of first dense layer outputs \n",
        "    dz[z<=0] = 0 # backpropagate through ReLU \n",
        "    dw3 = dz.dot(fc.T)\n",
        "    db3 = np.sum(dz, axis = 1).reshape(b3.shape)\n",
        "    dfc = w3.T.dot(dz) # loss gradients of fully-connected layer (pooling layer)\n",
        "    dpool = dfc.reshape(pooled.shape) # reshape fully connected into dimensions of pooling layer\n",
        "    dconv2 = maxpoolBackward(dpool, conv2, pool_f, pool_s) # backprop through the max-pooling layer(only neurons with highest activation in window get updated)\n",
        "    dconv2[conv2<=0] = 0 # backpropagate through ReLU\n",
        "    dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s) # backpropagate previous gradient through second convolutional layer.\n",
        "    dconv1[conv1<=0] = 0 # backpropagate through ReLU\n",
        "    dimage, df1, db1 = convolutionBackward(dconv1, image, f1, conv_s) # backpropagate previous gradient through first convolutional layer.\n",
        "    grads = [df1, df2, dw3, dw4, db1, db2, db3, db4]\n",
        "    return grads, loss\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63zJty_2ks6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adamGD(batch, num_classes, lr, dim, n_c, beta1, beta2, params, cost):\n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params \n",
        "    X = batch[:,0:-1] # get batch inputs\n",
        "    X = X.reshape(len(batch), n_c, dim, dim)\n",
        "    Y = batch[:,-1] # get batch labels\n",
        "    cost_ = 0\n",
        "    batch_size = len(batch)\n",
        "    # initialize gradients and momentum,RMS params\n",
        "    df1 = np.zeros(f1.shape)\n",
        "    df2 = np.zeros(f2.shape)\n",
        "    dw3 = np.zeros(w3.shape)\n",
        "    dw4 = np.zeros(w4.shape)\n",
        "    db1 = np.zeros(b1.shape)\n",
        "    db2 = np.zeros(b2.shape)\n",
        "    db3 = np.zeros(b3.shape)\n",
        "    db4 = np.zeros(b4.shape)\n",
        "    \n",
        "    v1 = np.zeros(f1.shape)\n",
        "    v2 = np.zeros(f2.shape)\n",
        "    v3 = np.zeros(w3.shape)\n",
        "    v4 = np.zeros(w4.shape)\n",
        "    bv1 = np.zeros(b1.shape)\n",
        "    bv2 = np.zeros(b2.shape)\n",
        "    bv3 = np.zeros(b3.shape)\n",
        "    bv4 = np.zeros(b4.shape)\n",
        "    \n",
        "    s1 = np.zeros(f1.shape)\n",
        "    s2 = np.zeros(f2.shape)\n",
        "    s3 = np.zeros(w3.shape)\n",
        "    s4 = np.zeros(w4.shape)\n",
        "    bs1 = np.zeros(b1.shape)\n",
        "    bs2 = np.zeros(b2.shape)\n",
        "    bs3 = np.zeros(b3.shape)\n",
        "    bs4 = np.zeros(b4.shape)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        \n",
        "        x = X[i]\n",
        "        y = np.eye(num_classes)[int(Y[i])].reshape(num_classes, 1) # convert label to one-hot\n",
        "        \n",
        "        # Collect Gradients for training example\n",
        "        grads, loss = conv(x, y, params, 1, 2, 2)\n",
        "        [df1_, df2_, dw3_, dw4_, db1_, db2_, db3_, db4_] = grads\n",
        "        \n",
        "        df1+=df1_\n",
        "        db1+=db1_\n",
        "        df2+=df2_\n",
        "        db2+=db2_\n",
        "        dw3+=dw3_\n",
        "        db3+=db3_\n",
        "        dw4+=dw4_\n",
        "        db4+=db4_\n",
        "\n",
        "        cost_+= loss\n",
        "\n",
        "    # Parameter Update  \n",
        "        \n",
        "    v1 = beta1*v1 + (1-beta1)*df1/batch_size # momentum update\n",
        "    s1 = beta2*s1 + (1-beta2)*(df1/batch_size)**2 # RMSProp update\n",
        "    f1 -= lr * v1/np.sqrt(s1+1e-7) # combine momentum and RMSProp to perform update with Adam\n",
        "    \n",
        "    bv1 = beta1*bv1 + (1-beta1)*db1/batch_size\n",
        "    bs1 = beta2*bs1 + (1-beta2)*(db1/batch_size)**2\n",
        "    b1 -= lr * bv1/np.sqrt(bs1+1e-7)\n",
        "   \n",
        "    v2 = beta1*v2 + (1-beta1)*df2/batch_size\n",
        "    s2 = beta2*s2 + (1-beta2)*(df2/batch_size)**2\n",
        "    f2 -= lr * v2/np.sqrt(s2+1e-7)\n",
        "                       \n",
        "    bv2 = beta1*bv2 + (1-beta1) * db2/batch_size\n",
        "    bs2 = beta2*bs2 + (1-beta2)*(db2/batch_size)**2\n",
        "    b2 -= lr * bv2/np.sqrt(bs2+1e-7)\n",
        "    \n",
        "    v3 = beta1*v3 + (1-beta1) * dw3/batch_size\n",
        "    s3 = beta2*s3 + (1-beta2)*(dw3/batch_size)**2\n",
        "    w3 -= lr * v3/np.sqrt(s3+1e-7)\n",
        "    \n",
        "    bv3 = beta1*bv3 + (1-beta1) * db3/batch_size\n",
        "    bs3 = beta2*bs3 + (1-beta2)*(db3/batch_size)**2\n",
        "    b3 -= lr * bv3/np.sqrt(bs3+1e-7)\n",
        "    \n",
        "    v4 = beta1*v4 + (1-beta1) * dw4/batch_size\n",
        "    s4 = beta2*s4 + (1-beta2)*(dw4/batch_size)**2\n",
        "    w4 -= lr * v4 / np.sqrt(s4+1e-7)\n",
        "    \n",
        "    bv4 = beta1*bv4 + (1-beta1)*db4/batch_size\n",
        "    bs4 = beta2*bs4 + (1-beta2)*(db4/batch_size)**2\n",
        "    b4 -= lr * bv4 / np.sqrt(bs4+1e-7)\n",
        "    \n",
        "\n",
        "    cost_ = cost_/batch_size\n",
        "    cost.append(cost_)\n",
        "\n",
        "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
        "    \n",
        "    return params, cost"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeV11YO7ks6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_classes = 2, lr = 0.01, beta1 = 0.95, beta2 = 0.99, img_dim = 200, img_depth = 1, f = 5, num_filt1 = 8, num_filt2 = 8, batch_size = 32, num_epochs = 2): \n",
        "    np.random.shuffle(train_data)\n",
        "\n",
        "    ## Initializing all the parameters\n",
        "    f1, f2, w3, w4 = (num_filt1,img_depth,f,f), (num_filt2 ,num_filt1,f,f), (128,73728), (2, 128)\n",
        "    f1 = initializeFilter(f1)\n",
        "    f2 = initializeFilter(f2)\n",
        "    w3 = initializeWeight(w3)\n",
        "    w4 = initializeWeight(w4)\n",
        "    \n",
        "    b1 = np.zeros((f1.shape[0],1))\n",
        "    b2 = np.zeros((f2.shape[0],1))\n",
        "    b3 = np.zeros((w3.shape[0],1))\n",
        "    b4 = np.zeros((w4.shape[0],1))\n",
        "    params = [f1, f2, w3, w4, b1, b2, b3, b4]\n",
        "    \n",
        "    cost = []\n",
        "\n",
        "    print(\"LR:\"+str(lr)+\", Batch Size:\"+str(batch_size))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        np.random.shuffle(train_data)\n",
        "        batches = [train_data[k:k + batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
        "\n",
        "        t = tqdm(batches)\n",
        "        for x,batch in enumerate(t):\n",
        "            params, cost = adamGD(batch, num_classes, lr, img_dim, img_depth, beta1, beta2, params, cost)\n",
        "            t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
        "        \n",
        "    return params, cost"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9j2PYvVks6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(image, f1, f2, w3, w4, b1, b2, b3, b4, conv_s = 1, pool_f = 2, pool_s = 2):\n",
        "    conv1 = convolution(image, f1, b1, conv_s) # convolution operation\n",
        "    conv1[conv1<=0] = 0 #relu activation\n",
        "    \n",
        "    conv2 = convolution(conv1, f2, b2, conv_s) # second convolution operation\n",
        "    conv2[conv2<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    pooled = maxpool(conv2, pool_f, pool_s) # maxpooling operation\n",
        "    (nf2, dim2, _) = pooled.shape\n",
        "    fc = pooled.reshape((nf2 * dim2 * dim2, 1)) # flatten pooled layer\n",
        "    \n",
        "    z = w3.dot(fc) + b3 # first dense layer\n",
        "    z[z<=0] = 0 # pass through ReLU non-linearity\n",
        "    \n",
        "    out = w4.dot(z) + b4 # second dense layer\n",
        "    probs = softmax(out) # predict class probabilities with the softmax activation function\n",
        "    return np.argmax(probs), np.max(probs)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxUi1ONZks6V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "3029b6fd-df1e-4d79-9064-ede8da036e70"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    params, cost = train(num_classes = 2,img_dim = 200)\n",
        "    [f1, f2, w3, w4, b1, b2, b3, b4] = params\n",
        "    \n",
        "    X = test_data[:,0:-1]\n",
        "    X = X.reshape(len(test_data), 1, 200, 200)\n",
        "    y = test_data[:,-1]\n",
        "\n",
        "    print(\"Computing accuracy over test set:\")\n",
        "    prediction = []\n",
        "    for i in range(len(X)):\n",
        "        pred, prob = predict(X[i], f1, f2, w3, w4, b1, b2, b3, b4)\n",
        "        prediction.append(pred)\n",
        "    confusion_matrix(y,prediction)\n",
        "    classification_report(y,prediction)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR:0.01, Batch Size:32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cost: 0.71:   0%|          | 0/5 [07:45<?, ?it/s]\u001b[A\n",
            "Cost: 0.71:  20%|██        | 1/5 [07:45<31:02, 465.70s/it]\u001b[A\n",
            "Cost: 251.85:  20%|██        | 1/5 [15:30<31:02, 465.70s/it]\u001b[A\n",
            "Cost: 251.85:  40%|████      | 2/5 [15:30<23:16, 465.43s/it]\u001b[A\n",
            "Cost: 12.57:  40%|████      | 2/5 [23:15<23:16, 465.43s/it] \u001b[A\n",
            "Cost: 12.57:  60%|██████    | 3/5 [23:15<15:30, 465.27s/it]\u001b[A\n",
            "Cost: 0.70:  60%|██████    | 3/5 [30:59<15:30, 465.27s/it] \u001b[A\n",
            "Cost: 0.70:  80%|████████  | 4/5 [30:59<07:45, 465.01s/it]\u001b[A\n",
            "Cost: 0.69:  80%|████████  | 4/5 [38:40<07:45, 465.01s/it]\u001b[A\n",
            "Cost: 0.69: 100%|██████████| 5/5 [38:40<00:00, 464.17s/it]\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "Cost: 0.68:   0%|          | 0/5 [07:44<?, ?it/s]\u001b[A\n",
            "Cost: 0.68:  20%|██        | 1/5 [07:44<30:58, 464.52s/it]\u001b[A\n",
            "Cost: 44.89:  20%|██        | 1/5 [15:30<30:58, 464.52s/it]\u001b[A\n",
            "Cost: 44.89:  40%|████      | 2/5 [15:30<23:14, 464.83s/it]\u001b[A\n",
            "Cost: 1.46:  40%|████      | 2/5 [23:14<23:14, 464.83s/it] \u001b[A\n",
            "Cost: 1.46:  60%|██████    | 3/5 [23:14<15:29, 464.85s/it]\u001b[A\n",
            "Cost: 1.04:  60%|██████    | 3/5 [30:55<15:29, 464.85s/it]\u001b[A\n",
            "Cost: 1.04:  80%|████████  | 4/5 [30:55<07:43, 463.44s/it]\u001b[A\n",
            "Cost: 0.84:  80%|████████  | 4/5 [38:38<07:43, 463.44s/it]\u001b[A\n",
            "Cost: 0.84: 100%|██████████| 5/5 [38:38<00:00, 463.73s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing accuracy over test set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy5mNXvks6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1672b3db-c469-44b3-afe9-7a04d9f167ad"
      },
      "source": [
        "print(confusion_matrix(y,prediction))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0 26]\n",
            " [ 0 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A5p_f0zks6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "e9cbec89-9d88-4e4a-e9a6-5bf77b1fd4dc"
      },
      "source": [
        "print(classification_report(y,prediction))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        26\n",
            "           1       0.35      1.00      0.52        14\n",
            "\n",
            "    accuracy                           0.35        40\n",
            "   macro avg       0.17      0.50      0.26        40\n",
            "weighted avg       0.12      0.35      0.18        40\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MO7UV3hks6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}